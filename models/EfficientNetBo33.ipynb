{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c735f3-3012-41f6-bbdf-aef469c2445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, cv2, hashlib\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"leapgestrecog\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "CLEAN_DIR = DATA_DIR / \"clean\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# -----------------------------\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ù…Ù† Kaggle\n",
    "print(\"ğŸ“¥ Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"gti-upm/leapgestrecog\")\n",
    "print(\"Downloaded to:\", path)\n",
    "\n",
    "if os.path.isdir(os.path.join(path, \"leapGestRecog\")):\n",
    "    src = os.path.join(path, \"leapGestRecog\")\n",
    "else:\n",
    "    src = path\n",
    "\n",
    "print(\"Using source:\", src)\n",
    "\n",
    "# Ù†Ø³Ø® Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "if not any(RAW_DIR.iterdir()):\n",
    "    shutil.copytree(src, RAW_DIR, dirs_exist_ok=True)\n",
    "    print(\"âœ… Raw data copied\")\n",
    "\n",
    "if not CLEAN_DIR.exists():\n",
    "    shutil.copytree(RAW_DIR, CLEAN_DIR)\n",
    "    print(\"âœ… Clean data folder created\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Clean data already exists\")\n",
    "\n",
    "# -----------------------------\n",
    "# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "def remove_corrupted(folder: Path):\n",
    "    print(\"\\nğŸ§¹ Removing corrupted images...\")\n",
    "    for cls in tqdm(sorted(os.listdir(folder))):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir(): continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "            except:\n",
    "                img_path.unlink(missing_ok=True)\n",
    "\n",
    "def fix_format(folder: Path, size=(224,224)):\n",
    "    print(\"\\nğŸ¨ Standardizing images...\")\n",
    "    for cls in tqdm(sorted(os.listdir(folder))):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir(): continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            try:\n",
    "                with Image.open(img_path) as im:\n",
    "                    im = im.convert(\"RGB\")\n",
    "                    im = im.resize(size)\n",
    "                    im.save(img_path)\n",
    "            except:\n",
    "                img_path.unlink(missing_ok=True)\n",
    "\n",
    "def remove_duplicates(folder: Path):\n",
    "    print(\"\\nğŸ—‘ï¸ Removing duplicates...\")\n",
    "    seen = set()\n",
    "    for cls in tqdm(sorted(os.listdir(folder))):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir(): continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir(): continue\n",
    "            try:\n",
    "                with open(img_path, \"rb\") as f:\n",
    "                    h = hashlib.md5(f.read()).hexdigest()\n",
    "                if h in seen:\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "                else:\n",
    "                    seen.add(h)\n",
    "            except:\n",
    "                img_path.unlink(missing_ok=True)\n",
    "\n",
    "def denoise(folder: Path):\n",
    "    print(\"\\nâœ¨ Denoising images...\")\n",
    "    for cls in tqdm(sorted(os.listdir(folder))):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir(): continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir(): continue\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None: continue\n",
    "                img = cv2.fastNlMeansDenoisingColored(img, None, 10,10,7,21)\n",
    "                cv2.imwrite(str(img_path), img)\n",
    "            except: pass\n",
    "\n",
    "def show_balance(folder: Path):\n",
    "    print(\"\\nğŸ“Š Class Balance:\")\n",
    "    for cls in sorted(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if cls_path.is_dir():\n",
    "            count = len([f for f in os.listdir(cls_path) if (cls_path / f).is_file()])\n",
    "            print(f\"{cls}: {count}\")\n",
    "\n",
    "# ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
    "remove_corrupted(CLEAN_DIR)\n",
    "fix_format(CLEAN_DIR, IMG_SIZE)\n",
    "remove_duplicates(CLEAN_DIR)\n",
    "denoise(CLEAN_DIR)\n",
    "show_balance(CLEAN_DIR)\n",
    "\n",
    "print(\"\\nâœ… Cleaning pipeline completed.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯ Generators Ù„Ù„ØªØ¯Ø±ÙŠØ¨\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=(0.8, 1.2),\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    str(CLEAN_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    str(CLEAN_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_gen.num_classes\n",
    "class_indices = train_gen.class_indices\n",
    "idx_to_class = {v:k for k,v in class_indices.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# Ø¨Ù†Ø§Ø¡ Ù…ÙˆØ¯ÙŠÙ„ EfficientNetB0\n",
    "base_model = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224,224,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=(224,224,3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"best_efficientnetb0.h5\", monitor=\"val_accuracy\", save_best_only=True, mode=\"max\"),\n",
    "    EarlyStopping(monitor=\"val_accuracy\", patience=8, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
    "history = model.fit(train_gen, epochs=25, validation_data=val_gen, callbacks=callbacks)\n",
    "\n",
    "# Fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history_ft = model.fit(train_gen, epochs=10, validation_data=val_gen, callbacks=callbacks)\n",
    "\n",
    "# -----------------------------\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
    "val_gen.reset()\n",
    "y_true = val_gen.classes\n",
    "y_pred_probs = model.predict(val_gen)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[idx_to_class[i] for i in range(num_classes)]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, cmap=\"Blues\", annot=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
