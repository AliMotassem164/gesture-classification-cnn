{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c735f3-3012-41f6-bbdf-aef469c2445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, cv2, hashlib\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# Paths setup\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"leapgestrecog\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "CLEAN_DIR = DATA_DIR / \"clean\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Download dataset\n",
    "print(\"Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"gti-upm/leapgestrecog\")\n",
    "print(\"Downloaded to:\", path)\n",
    "\n",
    "if os.path.isdir(os.path.join(path, \"leapGestRecog\")):\n",
    "    src = os.path.join(path, \"leapGestRecog\")\n",
    "else:\n",
    "    src = path\n",
    "\n",
    "print(\"Using source:\", src)\n",
    "\n",
    "# Copy dataset\n",
    "if not any(RAW_DIR.iterdir()):\n",
    "    shutil.copytree(src, RAW_DIR, dirs_exist_ok=True)\n",
    "    print(\"Raw data copied\")\n",
    "else:\n",
    "    print(\"Raw data already exists\")\n",
    "\n",
    "if not CLEAN_DIR.exists():\n",
    "    shutil.copytree(RAW_DIR, CLEAN_DIR, dirs_exist_ok=True)\n",
    "    print(\"Clean data folder created\")\n",
    "else:\n",
    "    print(\"Clean data already exists\")\n",
    "\n",
    "# -----------------------------\n",
    "# Cleaning functions\n",
    "def remove_corrupted(folder: Path):\n",
    "    print(\"\\nRemoving corrupted images...\")\n",
    "    for cls in tqdm(sorted(os.listdir(folder))):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None and img_path.is_file():\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "            except:\n",
    "                if img_path.is_file():\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "\n",
    "def fix_format(folder: Path, size=(224,224)):\n",
    "    print(\"\\nStandardizing images...\")\n",
    "    for cls in tqdm(sorted(os.listdir(folder))):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                with Image.open(img_path) as im:\n",
    "                    im = im.convert(\"RGB\")\n",
    "                    im = im.resize(size)\n",
    "                    im.save(img_path)\n",
    "            except:\n",
    "                if img_path.is_file():\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "\n",
    "def remove_duplicates(folder: Path):\n",
    "    print(\"\\nRemoving duplicates...\")\n",
    "    seen = set()\n",
    "    for cls in tqdm(sorted(os.listdir(folder))):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                with open(img_path, \"rb\") as f:\n",
    "                    h = hashlib.md5(f.read()).hexdigest()\n",
    "                if h in seen and img_path.is_file():\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "                else:\n",
    "                    seen.add(h)\n",
    "            except:\n",
    "                if img_path.is_file():\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "\n",
    "def denoise(folder: Path):\n",
    "    print(\"\\nDenoising images...\")\n",
    "    for cls in tqdm(sorted(os.listdir(folder))):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                img = cv2.fastNlMeansDenoisingColored(img, None, 10,10,7,21)\n",
    "                cv2.imwrite(str(img_path), img)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def show_balance(folder: Path):\n",
    "    print(\"\\nClass Balance:\")\n",
    "    for cls in sorted(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if cls_path.is_dir():\n",
    "            count = len([f for f in os.listdir(cls_path) if (cls_path / f).is_file()])\n",
    "            print(f\"{cls}: {count}\")\n",
    "\n",
    "# Run cleaning\n",
    "remove_corrupted(CLEAN_DIR)\n",
    "fix_format(CLEAN_DIR, IMG_SIZE)\n",
    "remove_duplicates(CLEAN_DIR)\n",
    "denoise(CLEAN_DIR)\n",
    "show_balance(CLEAN_DIR)\n",
    "\n",
    "print(\"\\nCleaning pipeline completed.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    zoom_range=0.25,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=(0.7, 1.3)\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    str(CLEAN_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    str(CLEAN_DIR),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_gen.num_classes\n",
    "class_indices = train_gen.class_indices\n",
    "idx_to_class = {v:k for k,v in class_indices.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# Build EfficientNetB0 model\n",
    "base_model = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224,224,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=(224,224,3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"best_efficientnetb0.h5\", monitor=\"val_accuracy\", save_best_only=True, mode=\"max\"),\n",
    "    EarlyStopping(monitor=\"val_accuracy\", patience=6, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Train model\n",
    "history = model.fit(train_gen, epochs=30, validation_data=val_gen, callbacks=callbacks)\n",
    "\n",
    "# Fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-40]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(5e-5),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history_ft = model.fit(train_gen, epochs=15, validation_data=val_gen, callbacks=callbacks)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate model\n",
    "val_gen.reset()\n",
    "y_true = val_gen.classes\n",
    "y_pred_probs = model.predict(val_gen)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[idx_to_class[i] for i in range(num_classes)]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, cmap=\"Blues\", annot=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Final accuracy\n",
    "loss, acc = model.evaluate(val_gen, verbose=0)\n",
    "print(f\"\\nFinal Validation Accuracy: {acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
