{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project structure (educational style)\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"leapgestrecog\"\n",
    "\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "CLEAN_DIR = DATA_DIR / \"clean\"\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Downloading dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kagglehub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“¥ Downloading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m path \u001b[38;5;241m=\u001b[39m kagglehub\u001b[38;5;241m.\u001b[39mdataset_download(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgti-upm/leapgestrecog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Dataset usually contains 'leapGestRecog'\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kagglehub' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“¥ Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"gti-upm/leapgestrecog\")\n",
    "print(\"Downloaded to:\", path)\n",
    "\n",
    "# Dataset usually contains 'leapGestRecog'\n",
    "if os.path.isdir(os.path.join(path, \"leapGestRecog\")):\n",
    "    src = os.path.join(path, \"leapGestRecog\")\n",
    "else:\n",
    "    src = path\n",
    "\n",
    "print(\"Using source:\", src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copy raw data once\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(RAW_DIR\u001b[38;5;241m.\u001b[39miterdir()):\n\u001b[1;32m----> 3\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopytree(src, RAW_DIR, dirs_exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Raw data copied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create clean data copy\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'src' is not defined"
     ]
    }
   ],
   "source": [
    "# Copy raw data once\n",
    "if not any(RAW_DIR.iterdir()):\n",
    "    shutil.copytree(src, RAW_DIR, dirs_exist_ok=True)\n",
    "    print(\"âœ… Raw data copied\")\n",
    "\n",
    "# Create clean data copy\n",
    "if not CLEAN_DIR.exists():\n",
    "    shutil.copytree(RAW_DIR, CLEAN_DIR)\n",
    "    print(\"âœ… Clean data folder created\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Clean data already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corrupted(folder: Path):\n",
    "    \"\"\"Remove images that cannot be read.\"\"\"\n",
    "    print(\"\\nğŸ§¹ Removing corrupted images...\")\n",
    "    for cls in tqdm(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "            except:\n",
    "                img_path.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_format(folder: Path, size=(224, 224)):\n",
    "    \"\"\"Convert images to RGB and resize.\"\"\"\n",
    "    print(\"\\nğŸ¨ Standardizing images...\")\n",
    "    for cls in tqdm(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = img.resize(size)\n",
    "                img.save(img_path)\n",
    "            except:\n",
    "                img_path.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(folder: Path):\n",
    "    \"\"\"Remove duplicate images using MD5 hash.\"\"\"\n",
    "    print(\"\\nğŸ—‘ï¸ Removing duplicates...\")\n",
    "    seen = set()\n",
    "    for cls in tqdm(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                h = hashlib.md5(f.read()).hexdigest()\n",
    "            if h in seen:\n",
    "                img_path.unlink(missing_ok=True)\n",
    "            else:\n",
    "                seen.add(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(folder: Path):\n",
    "    \"\"\"Apply denoising filter.\"\"\"\n",
    "    print(\"\\nâœ¨ Denoising images...\")\n",
    "    for cls in tqdm(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                img = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)\n",
    "                cv2.imwrite(str(img_path), img)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_balance(folder: Path):\n",
    "    \"\"\"Show number of images per class.\"\"\"\n",
    "    print(\"\\nğŸ“Š Class Balance:\")\n",
    "    for cls in sorted(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if cls_path.is_dir():\n",
    "            print(f\"{cls}: {len(os.listdir(cls_path))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§¹ Removing corrupted images...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data\\\\leapgestrecog\\\\clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run cleaning pipeline\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m remove_corrupted(CLEAN_DIR)\n\u001b[0;32m      3\u001b[0m fix_format(CLEAN_DIR, IMG_SIZE)\n\u001b[0;32m      4\u001b[0m remove_duplicates(CLEAN_DIR)\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mremove_corrupted\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove images that cannot be read.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ§¹ Removing corrupted images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tqdm(os\u001b[38;5;241m.\u001b[39mlistdir(folder)):\n\u001b[0;32m      5\u001b[0m     cls_path \u001b[38;5;241m=\u001b[39m folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_path\u001b[38;5;241m.\u001b[39mis_dir():\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data\\\\leapgestrecog\\\\clean'"
     ]
    }
   ],
   "source": [
    "# Run cleaning pipeline\n",
    "remove_corrupted(CLEAN_DIR)\n",
    "fix_format(CLEAN_DIR, IMG_SIZE)\n",
    "remove_duplicates(CLEAN_DIR)\n",
    "denoise(CLEAN_DIR)\n",
    "show_balance(CLEAN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EfficientNetB0\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, GlobalAveragePooling2D, Dropout\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"leapgestrecog\"\n",
    "CLEAN_DIR = DATA_DIR / \"clean\"\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# ğŸ§¹ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„ÙØ§Ø³Ø¯Ø© Ø£Ùˆ ØºÙŠØ± Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©\n",
    "def remove_corrupted(folder: Path):\n",
    "    print(\"\\nğŸ§¹ Removing corrupted images...\")\n",
    "    for cls in tqdm(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                img_path.unlink(missing_ok=True)\n",
    "                continue\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "            except Exception:\n",
    "                img_path.unlink(missing_ok=True)\n",
    "\n",
    "# ğŸ¨ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù„Ù€ RGB + Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ø¬ÙŠÙ…\n",
    "def fix_format(folder: Path, size=(224, 224)):\n",
    "    print(\"\\nğŸ¨ Standardizing images...\")\n",
    "    for cls in tqdm(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = img.resize(size)\n",
    "                img.save(img_path)\n",
    "            except Exception:\n",
    "                img_path.unlink(missing_ok=True)\n",
    "\n",
    "# ğŸ—‘ï¸ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… MD5\n",
    "def remove_duplicates(folder: Path):\n",
    "    print(\"\\nğŸ—‘ï¸ Removing duplicates...\")\n",
    "    seen = set()\n",
    "    for cls in tqdm(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                with open(img_path, \"rb\") as f:\n",
    "                    h = hashlib.md5(f.read()).hexdigest()\n",
    "                if h in seen:\n",
    "                    img_path.unlink(missing_ok=True)\n",
    "                else:\n",
    "                    seen.add(h)\n",
    "            except Exception:\n",
    "                img_path.unlink(missing_ok=True)\n",
    "\n",
    "# âœ¨ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ù…Ù† Ø§Ù„ØµÙˆØ±\n",
    "def denoise(folder: Path):\n",
    "    print(\"\\nâœ¨ Denoising images...\")\n",
    "    for cls in tqdm(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if not cls_path.is_dir():\n",
    "            continue\n",
    "        for file in os.listdir(cls_path):\n",
    "            img_path = cls_path / file\n",
    "            if img_path.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is not None:\n",
    "                    img = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)\n",
    "                    cv2.imwrite(str(img_path), img)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ğŸ“Š Ø¹Ø±Ø¶ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± ÙÙŠ ÙƒÙ„ ÙØ¦Ø© + ØªØ­Ø°ÙŠØ± Ù„Ùˆ ÙÙŠÙ‡ Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù†\n",
    "def show_balance(folder: Path):\n",
    "    print(\"\\nğŸ“Š Class Balance:\")\n",
    "    counts = {}\n",
    "    for cls in sorted(os.listdir(folder)):\n",
    "        cls_path = folder / cls\n",
    "        if cls_path.is_dir():\n",
    "            counts[cls] = len(os.listdir(cls_path))\n",
    "            print(f\"{cls}: {counts[cls]}\")\n",
    "    avg = sum(counts.values()) / len(counts)\n",
    "    for cls, count in counts.items():\n",
    "        if count < 0.5 * avg:\n",
    "            print(f\"âš ï¸ Warning: Class '{cls}' has fewer samples ({count}) than average ({avg:.0f})\")\n",
    "\n",
    "# ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ pipeline ÙƒÙ„Ù‡\n",
    "remove_corrupted(CLEAN_DIR)\n",
    "fix_format(CLEAN_DIR, IMG_SIZE)\n",
    "remove_duplicates(CLEAN_DIR)\n",
    "denoise(CLEAN_DIR)\n",
    "show_balance(CLEAN_DIR)\n",
    "\n",
    "# ğŸ”¥ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ datagen Ù…Ø¹ augmentations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    CLEAN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset=\"training\",\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    CLEAN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset=\"validation\",\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# ğŸ§  Ø¨Ù†Ø§Ø¡ EfficientNetB0 Ù…Ø¹ transfer learning\n",
    "base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=IMG_SIZE + (3,))\n",
    "base_model.trainable = False  # Ù†Ø¬Ù…Ù‘Ø¯ Ø§Ù„Ù€ feature extractor ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "preds = Dense(train_gen.num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=preds)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# ØªØ¯Ø±ÙŠØ¨ Ø£ÙˆÙ„ÙŠ\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Fine-tuning: ÙÙƒ ØªØ¬Ù…ÙŠØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
    "model.save(\"efficientnet_gesture.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 39466,
     "sourceId": 61155,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
